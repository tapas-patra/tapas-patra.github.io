api.js:
// Initialize the Supabase client
let supabaseClient = null;

// Rate limiting management
const rateLimitManager = {
    lastRequestTime: 0,
    minRequestInterval: CONFIG.RATE_LIMIT.MIN_INTERVAL,
    retryCount: 0,
    maxRetries: CONFIG.RATE_LIMIT.MAX_RETRIES,
    isWaitingForRateLimit: false,

    // Get delay time for exponential backoff
    getRetryDelay() {
        // Exponential backoff with jitter: 2^retryCount * baseDelay + random jitter
        return Math.min(
            (Math.pow(2, this.retryCount) * CONFIG.RATE_LIMIT.BASE_DELAY) + (Math.random() * 1000),
            CONFIG.RATE_LIMIT.MAX_DELAY
        );
    },

    // Reset retry count
    reset() {
        this.retryCount = 0;
        this.isWaitingForRateLimit = false;
    },

    // Increment retry count
    incrementRetry() {
        this.retryCount++;
        return this.retryCount <= this.maxRetries;
    },

    // Ensure we're not sending requests too quickly
    async enforceRateLimit() {
        const now = Date.now();
        const timeSinceLastRequest = now - this.lastRequestTime;

        if (timeSinceLastRequest < this.minRequestInterval) {
            // Wait before proceeding
            const waitTime = this.minRequestInterval - timeSinceLastRequest;
            await new Promise(resolve => setTimeout(resolve, waitTime));
        }

        this.lastRequestTime = Date.now();
    }
};

// Cache management
const responseCache = {
    // LRU (Least Recently Used) cache with a max size
    maxSize: CONFIG.CACHE.MAX_SIZE,
    items: new Map(),
    enabled: CONFIG.CACHE.ENABLED,
    ttl: CONFIG.CACHE.TTL,

    // Get a cached response
    get(query) {
        if (!this.enabled) return null;

        const normalizedQuery = query.toLowerCase().trim();
        const item = this.items.get(normalizedQuery);

        if (item) {
            // Check if cache entry has expired
            if (Date.now() - item.timestamp > this.ttl) {
                this.items.delete(normalizedQuery);
                return null;
            }

            // Move this item to the end (most recently used)
            this.items.delete(normalizedQuery);
            this.items.set(normalizedQuery, item);

            console.log('Cache hit for query:', normalizedQuery);
            return item.response;
        }

        return null; // Cache miss
    },

    // Store a response in the cache
    set(query, response) {
        if (!this.enabled) return;

        const normalizedQuery = query.toLowerCase().trim();

        // Check if cache is full and remove oldest item if needed
        if (this.items.size >= this.maxSize) {
            const oldestKey = this.items.keys().next().value;
            this.items.delete(oldestKey);
        }

        // Add the new item
        this.items.set(normalizedQuery, {
            response,
            timestamp: Date.now()
        });

        console.log('Cached response for query:', normalizedQuery);
    },

    // Clear cache
    clear() {
        this.items.clear();
    }
};

// Fallback responses for when the API is unavailable
const fallbackResponses = {
    greeting: "Hello! I'm glad you're visiting my portfolio. How can I help you today?",
    error: "I apologize, but my AI assistant is having trouble accessing information about me right now. Please try again in a moment.",
    general: "I'd be happy to tell you more about my experience and projects when my AI assistant's connection is restored. Please try a different question or try again shortly.",
    skills: "I have experience in web development, including technologies like JavaScript, React, and Node.js. I can provide more specific details when my AI connection improves.",
    experience: "I have professional experience in software development. I can share more details about my work history when my AI connection improves.",
    projects: "I've worked on several interesting projects in my career. I can tell you more about them when my AI connection improves."
};

// Function to detect casual conversation
function isCasualConversation(query) {
    const casualPatterns = [
        /^hi+\s*$/i,
        /^hello+\s*$/i,
        /^hey+\s*$/i,
        /^how are you/i,
        /^what's up/i,
        /^good morning/i,
        /^good afternoon/i,
        /^good evening/i,
        /^nice to meet you/i,
        /^thanks/i,
        /^thank you/i,
        /^ok+\s*$/i,
        /^okay+\s*$/i,
        /^cool+\s*$/i,
        /^bye+\s*$/i,
        /^goodbye/i,
        /^see you/i
    ];

    return casualPatterns.some(pattern => pattern.test(query.trim()));
}

// Initialize API connections
async function initializeConnections() {
    try {
        // Check if Supabase is available
        if (typeof supabase === 'undefined') {
            throw new Error('Supabase client is not loaded');
        }

        // Initialize Supabase client
        supabaseClient = supabase.createClient(
            CONFIG.SUPABASE_URL,
            CONFIG.SUPABASE_ANON_KEY
        );

        // Verify the connection works
        const { data, error } = await supabaseClient.from('documents').select('id').limit(1);

        if (error) {
            throw new Error(`Supabase connection error: ${error.message}`);
        }

        return true;
    } catch (error) {
        console.error('Failed to initialize connections:', error);
        return false;
    }
}

// Extract potential keywords from a query
function extractKeywords(query) {
    // Remove common words, keep only potential keywords
    const stopWords = ["a", "an", "the", "and", "or", "but", "in", "on", "at", "to", "for", "with", "about", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "do", "does", "did", "can", "could", "will", "would", "shall", "should", "may", "might", "must"];

    return query.toLowerCase()
        .replace(/[^\w\s]/g, '') // Remove punctuation
        .split(/\s+/) // Split by whitespace
        .filter(word =>
            word.length > 2 && // Word must be longer than 2 chars
            !stopWords.includes(word) // Word must not be a stop word
        );
}

// Generate embeddings using Mistral with rate limit handling
async function generateEmbedding(text) {
    // Reset retry counter at the start of a new request
    rateLimitManager.reset();

    // Keep trying until we hit max retries
    while (true) {
        try {
            // Enforce rate limiting
            await rateLimitManager.enforceRateLimit();

            const response = await fetch('https://api.mistral.ai/v1/embeddings', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${CONFIG.MISTRAL_API_KEY}`
                },
                body: JSON.stringify({
                    model: CONFIG.MISTRAL_EMBEDDING_MODEL,
                    input: text
                })
            });

            if (response.status === 429) {
                // Handle rate limit
                const retryAfter = response.headers.get('Retry-After') ||
                                   rateLimitManager.getRetryDelay() / 1000;

                console.warn(`Rate limited. Retrying after ${retryAfter} seconds.`);

                // Check if we should retry
                if (!rateLimitManager.incrementRetry()) {
                    throw new Error('Maximum retry attempts reached for rate limiting');
                }

                // Wait before retrying
                const delayMs = retryAfter * 1000 || rateLimitManager.getRetryDelay();
                rateLimitManager.isWaitingForRateLimit = true;

                // Send a rate limit event for UI updates
                const rateEvent = new CustomEvent('ratelimit', {
                    detail: { waitTime: delayMs, retryCount: rateLimitManager.retryCount }
                });
                document.dispatchEvent(rateEvent);

                await new Promise(resolve => setTimeout(resolve, delayMs));
                rateLimitManager.isWaitingForRateLimit = false;

                // Try again
                continue;
            }

            if (!response.ok) {
                const errorData = await response.json().catch(() => ({}));
                throw new Error(`Mistral API Error: ${errorData.error?.message || response.statusText}`);
            }

            const data = await response.json();
            return data.data[0].embedding;

        } catch (error) {
            // If it's not a rate limit error or we've exceeded retries, throw
            if (!rateLimitManager.isWaitingForRateLimit || rateLimitManager.retryCount > rateLimitManager.maxRetries) {
                console.error('Error generating embedding:', error);
                throw error;
            }
            // Otherwise, the loop will continue due to the continue statement above
        }
    }
}

// Get relevant documents using both vector search and keyword filtering
async function getRelevantDocuments(queryEmbedding, keywords) {
    try {
        // Try hybrid search if available
        try {
            const { data, error } = await supabaseClient.rpc('hybrid_search', {
                query_text: keywords.join(' '),
                query_embedding: queryEmbedding,
                match_threshold: CONFIG.SIMILARITY_THRESHOLD,
                match_count: CONFIG.MAX_DOCUMENTS
            });

            if (!error && data && data.length > 0) {
                return data;
            }
        } catch (hybridError) {
            console.log('Hybrid search not available, falling back to vector search');
        }

        // Fall back to vector search
        const vectorResults = await getSimilarDocuments(queryEmbedding);

        // If we got good results, return them
        if (vectorResults && vectorResults.length >= 3) {
            return vectorResults;
        }

        // If vector search didn't return enough results, try keyword search
        let keywordResults = [];

        for (const keyword of keywords) {
            // Search for the keyword in the content
            const { data, error } = await supabaseClient
                .from('documents')
                .select('id, content, metadata')
                .ilike('content', `%${keyword}%`)
                .limit(3);

            if (!error && data) {
                // Add a fake similarity score for ranking
                const scoredResults = data.map(doc => ({
                    ...doc,
                    similarity: 0.6 // Lower than typical vector search results
                }));

                keywordResults = [...keywordResults, ...scoredResults];
            }
        }

        // Deduplicate results
        const uniqueResults = Array.from(
            new Map(keywordResults.map(item => [item.id, item])).values()
        );

        // Combine vector and keyword results, removing duplicates
        const allResults = [...vectorResults];

        for (const keywordResult of uniqueResults) {
            if (!allResults.some(item => item.id === keywordResult.id)) {
                allResults.push(keywordResult);
            }
        }

        return allResults;
    } catch (error) {
        console.error('Error retrieving documents:', error);
        throw error;
    }
}

// Fetch similar documents from Supabase
async function getSimilarDocuments(embedding) {
    try {
        const { data, error } = await supabaseClient.rpc('match_documents', {
            query_embedding: embedding,
            match_threshold: CONFIG.SIMILARITY_THRESHOLD,
            match_count: CONFIG.MAX_DOCUMENTS
        });

        if (error) {
            throw error;
        }

        return data || [];
    } catch (error) {
        console.error('Error fetching similar documents:', error);
        throw error;
    }
}

// Generate chat completion with Mistral AI - with rate limit handling
async function generateChatCompletion(messages) {
    // Reset retry counter at the start of a new request
    rateLimitManager.reset();

    // Keep trying until we hit max retries
    while (true) {
        try {
            // Enforce rate limiting
            await rateLimitManager.enforceRateLimit();

            const response = await fetch('https://api.mistral.ai/v1/chat/completions', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${CONFIG.MISTRAL_API_KEY}`
                },
                body: JSON.stringify({
                    model: CONFIG.MISTRAL_COMPLETION_MODEL,
                    messages: messages
                })
            });

            if (response.status === 429) {
                // Handle rate limit
                const retryAfter = response.headers.get('Retry-After') ||
                                   rateLimitManager.getRetryDelay() / 1000;

                console.warn(`Rate limited. Retrying after ${retryAfter} seconds.`);

                // Check if we should retry
                if (!rateLimitManager.incrementRetry()) {
                    throw new Error('Maximum retry attempts reached for rate limiting');
                }

                // Wait before retrying
                const delayMs = retryAfter * 1000 || rateLimitManager.getRetryDelay();
                rateLimitManager.isWaitingForRateLimit = true;

                // Send a rate limit event for UI updates
                const rateEvent = new CustomEvent('ratelimit', {
                    detail: { waitTime: delayMs, retryCount: rateLimitManager.retryCount }
                });
                document.dispatchEvent(rateEvent);

                await new Promise(resolve => setTimeout(resolve, delayMs));
                rateLimitManager.isWaitingForRateLimit = false;

                // Try again
                continue;
            }

            if (!response.ok) {
                const errorData = await response.json().catch(() => ({}));
                throw new Error(`Mistral API Error: ${errorData.error?.message || response.statusText}`);
            }

            const data = await response.json();
            return data.choices[0].message.content;

        } catch (error) {
            // If it's not a rate limit error or we've exceeded retries, throw
            if (!rateLimitManager.isWaitingForRateLimit || rateLimitManager.retryCount > rateLimitManager.maxRetries) {
                console.error('Error generating chat completion:', error);
                throw error;
            }
            // Otherwise, the loop will continue due to the continue statement above
        }
    }
}

// RAG response generation - with caching, fallbacks, and casual conversation handling
async function generateRAGResponse(userQuery) {
    // Check if this is casual conversation
    if (isCasualConversation(userQuery)) {
        // Handle casual conversation differently
        const messages = [
            {
                role: "system",
                content: "You are a friendly AI assistant on a portfolio website. Respond naturally to casual conversation without mentioning portfolio information unless specifically asked."
            },
            {
                role: "user",
                content: userQuery
            }
        ];

        try {
            // Check cache first
            const cachedResponse = responseCache.get(userQuery);
            if (cachedResponse) return cachedResponse;

            const response = await generateChatCompletion(messages);
            responseCache.set(userQuery, response);
            return response;
        } catch (error) {
            console.error("Error in casual conversation:", error);

            // Return appropriate fallback
            if (userQuery.toLowerCase().includes("hi") || userQuery.toLowerCase().includes("hello") || userQuery.toLowerCase().includes("hey")) {
                return fallbackResponses.greeting;
            } else {
                return "Nice chatting with you! Let me know if you'd like to know anything about my work or projects.";
            }
        }
    }

    // For non-casual queries, proceed with regular RAG
    // Check cache first
    const cachedResponse = responseCache.get(userQuery);
    if (cachedResponse) {
        return cachedResponse;
    }

    try {
        // Extract keywords for potential keyword search
        const keywords = extractKeywords(userQuery);

        // Generate embedding for the query
        const queryEmbedding = await generateEmbedding(userQuery);

        // Retrieve relevant documents
        const relevantDocs = await getRelevantDocuments(queryEmbedding, keywords);

        // Prepare context from retrieved documents
        let context = "";
        if (relevantDocs && relevantDocs.length > 0) {
            // Sort documents by relevance score
            relevantDocs.sort((a, b) => b.similarity - a.similarity);

            // Take only the top documents until we reach a reasonable context size
            let contextSize = 0;
            const maxContextSize = CONFIG.CONTEXT.MAX_SIZE;
            const selectedDocs = [];

            for (const doc of relevantDocs) {
                if (contextSize + doc.content.length <= maxContextSize) {
                    selectedDocs.push(doc);
                    contextSize += doc.content.length;
                } else {
                    break;
                }
            }

            // Create the context string
            context = selectedDocs.map(doc => doc.content).join("\n\n");
        } else {
            context = "No relevant information found.";
        }

        // Generate a response using the context and query
        const messages = [
            {
                role: "system",
                content: CONFIG.SYSTEM_PROMPT
            },
            {
                role: "user",
                content: `Here is information about me:\n\n${context}\n\nBased only on this information, please answer the following question as if you are me: ${userQuery}`
            }
        ];

        const response = await generateChatCompletion(messages);

        // Cache the response before returning
        responseCache.set(userQuery, response);

        return response;
    } catch (error) {
        console.error("Error in RAG process:", error);

        // Try to provide a relevant fallback response
        const lowercaseQuery = userQuery.toLowerCase();

        if (lowercaseQuery.includes("hello") || lowercaseQuery.includes("hi") || lowercaseQuery.includes("hey")) {
            return fallbackResponses.greeting;
        } else if (lowercaseQuery.includes("skill") || lowercaseQuery.includes("technology") || lowercaseQuery.includes("tech stack")) {
            return fallbackResponses.skills;
        } else if (lowercaseQuery.includes("experience") || lowercaseQuery.includes("work") || lowercaseQuery.includes("job")) {
            return fallbackResponses.experience;
        } else if (lowercaseQuery.includes("project") || lowercaseQuery.includes("portfolio") || lowercaseQuery.includes("build")) {
            return fallbackResponses.projects;
        } else {
            return fallbackResponses.general;
        }
    }
}

app.js:
// DOM References
const loadingOverlay = document.getElementById('loading-overlay');
const chatMessages = document.getElementById('chat-messages');
const userInput = document.getElementById('user-input');
const sendButton = document.getElementById('send-button');
const clearChatButton = document.getElementById('clear-chat');
const introPanel = document.getElementById('intro-panel');
const suggestionChips = document.querySelectorAll('.suggestion-chip');

// Track initialization state
let isInitialized = false;

// Initialize the application
async function initializeApp() {
    // Show loading overlay
    loadingOverlay.classList.remove('hidden');

    try {
        // Initialize API connections
        const success = await initializeConnections();

        if (!success) {
            throw new Error('Failed to initialize connections');
        }

        // Initialize is complete
        isInitialized = true;

        // Hide loading overlay
        loadingOverlay.classList.add('hidden');

        // Add welcome message
        setTimeout(() => {
            addBotMessage("Hi there! How can I help you today?");
        }, 500);

    } catch (error) {
        console.error('Initialization error:', error);
        loadingOverlay.classList.add('hidden');

        // Show error message
        addErrorMessage("I'm having trouble connecting to my knowledge base. Please try again later.");
    }
}

// Add event listeners
function setupEventListeners() {
    // Send button click
    sendButton.addEventListener('click', handleUserMessage);

    // Enter key in textarea
    userInput.addEventListener('keydown', (e) => {
        if (e.key === 'Enter' && !e.shiftKey) {
            e.preventDefault();
            handleUserMessage();
        }
    });

    // Input changes (for enabling/disabling send button)
    userInput.addEventListener('input', () => {
        // Adjust textarea height based on content
        userInput.style.height = 'auto';
        userInput.style.height = `${Math.min(userInput.scrollHeight, 120)}px`;

        // Enable/disable send button based on input
        sendButton.disabled = userInput.value.trim() === '';
    });

    // Clear chat button
    clearChatButton.addEventListener('click', () => {
        // Clear messages
        chatMessages.innerHTML = '';

        // Show intro panel
        introPanel.classList.remove('hidden');

        // Clear cache
        if (responseCache && typeof responseCache.clear === 'function') {
            responseCache.clear();
        }

        // Add welcome message
        setTimeout(() => {
            addBotMessage("I've cleared our conversation. How else can I help you?");
        }, 300);
    });

    // Suggestion chips
    suggestionChips.forEach(chip => {
        chip.addEventListener('click', () => {
            // Hide intro panel
            introPanel.classList.add('hidden');

            // Get query from data attribute
            const query = chip.getAttribute('data-query');

            // Set input value
            userInput.value = query;

            // Trigger input event to adjust height and enable send button
            userInput.dispatchEvent(new Event('input'));

            // Send message
            handleUserMessage();
        });
    });

    // Rate limit event listener
    document.addEventListener('ratelimit', (event) => {
        const { waitTime, retryCount } = event.detail;
        const seconds = Math.ceil(waitTime / 1000);

        // Update the thinking indicator to show rate limit info
        const existingThinking = document.querySelector('.message.thinking');

        if (existingThinking) {
            existingThinking.innerHTML = `
                <p class="rate-limit-message">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                        <circle cx="12" cy="12" r="10"></circle>
                        <line x1="12" y1="8" x2="12" y2="12"></line>
                        <line x1="12" y1="16" x2="12.01" y2="16"></line>
                    </svg>
                    Rate limit reached. Waiting ${seconds} seconds before retry ${retryCount}/${CONFIG.RATE_LIMIT.MAX_RETRIES}...
                </p>
            `;
        }
    });
}

// Handle user message
async function handleUserMessage() {
    const message = userInput.value.trim();

    // Don't process empty messages
    if (!message) return;

    // Don't process if not initialized
    if (!isInitialized) {
        addErrorMessage("I'm still getting ready. Please wait a moment and try again.");
        return;
    }

    // Hide intro panel if visible
    introPanel.classList.add('hidden');

    // Add user message to chat
    addUserMessage(message);

    // Clear input and reset height
    userInput.value = '';
    userInput.style.height = 'auto';
    userInput.focus();

    // Disable send button
    sendButton.disabled = true;

    // Show thinking indicator
    const thinkingIndicator = addThinkingIndicator();

    try {
        // Generate response
        const response = await generateRAGResponse(message);

        // Remove thinking indicator
        chatMessages.removeChild(thinkingIndicator);

        // Add bot response
        addBotMessage(response);
    } catch (error) {
        // Remove thinking indicator
        chatMessages.removeChild(thinkingIndicator);

        // Add error message
        addErrorMessage("I'm having trouble generating a response right now. Please try again later.");
        console.error("Error generating response:", error);
    }
}

// Add user message to chat
function addUserMessage(message) {
    const messageElement = document.createElement('div');
    messageElement.className = 'message message-user';

    // Format the message text (replacing URLs, etc)
    messageElement.innerHTML = formatMessageText(message);

    // Add to chat and scroll to bottom
    chatMessages.appendChild(messageElement);
    scrollToBottom();
}

// Add bot message to chat
function addBotMessage(message) {
    const messageElement = document.createElement('div');
    messageElement.className = 'message message-bot';

    // Process markdown-like formatting
    const formattedMessage = formatMessageText(message);

    messageElement.innerHTML = formattedMessage;
    chatMessages.appendChild(messageElement);
    scrollToBottom();
}

// Add error message
function addErrorMessage(message) {
    const messageElement = document.createElement('div');
    messageElement.className = 'message message-bot error';
    messageElement.innerHTML = `<p>⚠️ ${message}</p>`;
    chatMessages.appendChild(messageElement);
    scrollToBottom();
}

// Add thinking indicator
function addThinkingIndicator() {
    const thinkingElement = document.createElement('div');
    thinkingElement.className = 'message message-bot thinking';

    const dotsContainer = document.createElement('div');
    dotsContainer.className = 'thinking-dots';

    for (let i = 0; i < 3; i++) {
        const dot = document.createElement('div');
        dot.className = 'thinking-dot';
        dotsContainer.appendChild(dot);
    }

    thinkingElement.appendChild(dotsContainer);
    chatMessages.appendChild(thinkingElement);
    scrollToBottom();

    return thinkingElement;
}

// Format message text (handles code blocks, links, etc.)
function formatMessageText(text) {
    // Replace code blocks with <pre><code> elements
    let formattedText = text.replace(/```([\s\S]*?)```/g, '<pre><code>$1</code></pre>');

    // Replace inline code with <code> elements
    formattedText = formattedText.replace(/`([^`]+)`/g, '<code>$1</code>');

    // Auto-link URLs
    formattedText = formattedText.replace(
        /(https?:\/\/[^\s]+)/g,
        '<a href="$1" target="_blank" rel="noopener noreferrer">$1</a>'
    );

    // Replace newlines with <br>
    formattedText = formattedText.replace(/\n/g, '<br>');

    return formattedText;
}

// Scroll to bottom of messages
function scrollToBottom() {
    chatMessages.scrollTop = chatMessages.scrollHeight;
}

// Initialize the app when the page loads
document.addEventListener('DOMContentLoaded', () => {
    setupEventListeners();
    initializeApp();
});


config.js:
const CONFIG = {
    SUPABASE_URL: "https://databse.supabase.co",
    SUPABASE_ANON_KEY: "supabase-key",
    MISTRAL_API_KEY: "mistral-key",
    MISTRAL_EMBEDDING_MODEL: "mistral-embed",
    MISTRAL_COMPLETION_MODEL: "open-mistral-7b",
    SIMILARITY_THRESHOLD: 0.6,
    MAX_DOCUMENTS: 3, // Reduced from 5 to decrease API load
    SYSTEM_PROMPT: "You are an AI assistant representing me on my personal portfolio website. Speak in first person as if you are me when discussing my skills, projects, and experience (e.g., 'I have experience with...' not 'The portfolio owner has...'). For general questions or casual conversation, respond naturally without forcing portfolio information. Be friendly, conversational, and helpful. If you don't know something specific about me, just be honest and offer to help with something else.",
    RATE_LIMIT: {
        MAX_RETRIES: 3,
        MIN_INTERVAL: 1500, // Minimum milliseconds between requests
        BASE_DELAY: 2000,   // Base delay for exponential backoff
        MAX_DELAY: 30000    // Maximum delay (30 seconds)
    },
    CACHE: {
        ENABLED: true,
        MAX_SIZE: 20,       // Maximum number of cached responses
        TTL: 86400000       // Time to live: 24 hours in milliseconds
    },
    CONTEXT: {
        MAX_SIZE: 4000      // Maximum context size in characters
    }
};

